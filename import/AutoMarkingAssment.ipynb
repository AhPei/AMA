{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "# from collections import defaultdict\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12976.000000</td>\n",
       "      <td>12976.000000</td>\n",
       "      <td>12976.000000</td>\n",
       "      <td>12976.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>12976.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10295.395808</td>\n",
       "      <td>4.179485</td>\n",
       "      <td>4.127158</td>\n",
       "      <td>4.137408</td>\n",
       "      <td>37.828125</td>\n",
       "      <td>6.800247</td>\n",
       "      <td>3.333889</td>\n",
       "      <td>3.330556</td>\n",
       "      <td>3.333889</td>\n",
       "      <td>2.444154</td>\n",
       "      <td>...</td>\n",
       "      <td>2.635689</td>\n",
       "      <td>2.710297</td>\n",
       "      <td>3.777317</td>\n",
       "      <td>3.589212</td>\n",
       "      <td>3.945312</td>\n",
       "      <td>3.890625</td>\n",
       "      <td>4.078125</td>\n",
       "      <td>3.992188</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6309.074105</td>\n",
       "      <td>2.136913</td>\n",
       "      <td>4.212544</td>\n",
       "      <td>4.264330</td>\n",
       "      <td>5.240829</td>\n",
       "      <td>8.970705</td>\n",
       "      <td>0.729103</td>\n",
       "      <td>0.726807</td>\n",
       "      <td>0.729103</td>\n",
       "      <td>1.211730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.142566</td>\n",
       "      <td>1.045795</td>\n",
       "      <td>0.689401</td>\n",
       "      <td>0.693256</td>\n",
       "      <td>0.643668</td>\n",
       "      <td>0.630390</td>\n",
       "      <td>0.622535</td>\n",
       "      <td>0.509687</td>\n",
       "      <td>0.538845</td>\n",
       "      <td>0.603417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4438.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10044.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15681.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21633.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           essay_id     essay_set  rater1_domain1  rater2_domain1  \\\n",
       "count  12976.000000  12976.000000    12976.000000    12976.000000   \n",
       "mean   10295.395808      4.179485        4.127158        4.137408   \n",
       "std     6309.074105      2.136913        4.212544        4.264330   \n",
       "min        1.000000      1.000000        0.000000        0.000000   \n",
       "25%     4438.750000      2.000000        2.000000        2.000000   \n",
       "50%    10044.500000      4.000000        3.000000        3.000000   \n",
       "75%    15681.250000      6.000000        4.000000        4.000000   \n",
       "max    21633.000000      8.000000       30.000000       30.000000   \n",
       "\n",
       "       rater3_domain1  domain1_score  rater1_domain2  rater2_domain2  \\\n",
       "count      128.000000   12976.000000     1800.000000     1800.000000   \n",
       "mean        37.828125       6.800247        3.333889        3.330556   \n",
       "std          5.240829       8.970705        0.729103        0.726807   \n",
       "min         20.000000       0.000000        1.000000        1.000000   \n",
       "25%         36.000000       2.000000        3.000000        3.000000   \n",
       "50%         40.000000       3.000000        3.000000        3.000000   \n",
       "75%         40.000000       8.000000        4.000000        4.000000   \n",
       "max         50.000000      60.000000        4.000000        4.000000   \n",
       "\n",
       "       domain2_score  rater1_trait1  ...  rater2_trait3  rater2_trait4  \\\n",
       "count    1800.000000    2292.000000  ...    2292.000000    2292.000000   \n",
       "mean        3.333889       2.444154  ...       2.635689       2.710297   \n",
       "std         0.729103       1.211730  ...       1.142566       1.045795   \n",
       "min         1.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         3.000000       2.000000  ...       2.000000       2.000000   \n",
       "50%         3.000000       2.000000  ...       2.000000       3.000000   \n",
       "75%         4.000000       3.000000  ...       4.000000       3.000000   \n",
       "max         4.000000       6.000000  ...       6.000000       6.000000   \n",
       "\n",
       "       rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "count     723.000000     723.000000     128.000000     128.000000   \n",
       "mean        3.777317       3.589212       3.945312       3.890625   \n",
       "std         0.689401       0.693256       0.643668       0.630390   \n",
       "min         1.000000       1.000000       2.000000       2.000000   \n",
       "25%         3.000000       3.000000       4.000000       4.000000   \n",
       "50%         4.000000       4.000000       4.000000       4.000000   \n",
       "75%         4.000000       4.000000       4.000000       4.000000   \n",
       "max         6.000000       6.000000       6.000000       6.000000   \n",
       "\n",
       "       rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "count     128.000000     128.000000     128.000000     128.000000  \n",
       "mean        4.078125       3.992188       3.843750       3.617188  \n",
       "std         0.622535       0.509687       0.538845       0.603417  \n",
       "min         2.000000       3.000000       2.000000       2.000000  \n",
       "25%         4.000000       4.000000       4.000000       3.000000  \n",
       "50%         4.000000       4.000000       4.000000       4.000000  \n",
       "75%         4.000000       4.000000       4.000000       4.000000  \n",
       "max         6.000000       6.000000       5.000000       5.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe = pd.read_csv('all_essaysets.csv', encoding = 'latin-1')\n",
    "dataframe = pd.read_csv('training.tsv', encoding = 'latin-1', sep='\\t')\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting which set to be used 1-8\n",
    "# in order to combine them all assign set number to 9\n",
    "def select_set(dataframe,setNumber):\n",
    "    if setNumber == 9:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==1]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "        for i in range(1,9):\n",
    "            dataframe2 = dataframe[dataframe.essay_set == i]\n",
    "            texts = texts.append(dataframe2['essay'])\n",
    "            s = dataframe2['domain1_score']\n",
    "            s = s.apply(lambda x: (x*3)/s.max())\n",
    "            scores = scores.append(s)\n",
    "    else:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==setNumber]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram plot of scores and average score\n",
    "def get_hist_avg(scores,bin_count):\n",
    "    print(sum(scores)/len(scores))\n",
    "    scores.hist(bins=bin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average word length for a text\n",
    "def avg_word_len(text):\n",
    "    clean_essay = re.sub(r'\\W', ' ', text)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    total = 0\n",
    "    for word in words:\n",
    "        total = total + len(word)\n",
    "    average = total / len(words)\n",
    "    \n",
    "    return average\n",
    "\n",
    "# word count in a given text\n",
    "def word_count(text):\n",
    "    clean_essay = re.sub(r'\\W', ' ', text)\n",
    "    return len(nltk.word_tokenize(clean_essay))\n",
    "\n",
    "# char count in a given text\n",
    "def char_count(text):\n",
    "    return len(re.sub(r'\\s', '', str(text).lower()))\n",
    "\n",
    "# sentence count in a given text\n",
    "def sent_count(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "#tokenization of texts to sentences\n",
    "def sent_tokenize(text):\n",
    "    stripped_essay = text.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "            tokens = nltk.word_tokenize(clean_sentence)\n",
    "            tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# lemma, noun, adjective, verb, adverb count for a given text\n",
    "\n",
    "def count_lemmas(text):\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0   \n",
    "    lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return noun_count, adj_count, verb_count, adv_count, lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_word(text):\n",
    "    text = \"\".join([ch.lower() for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misspell_count(text):\n",
    "    spell = SpellChecker()\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(token_word(text))\n",
    "    #print(misspelled)\n",
    "    return len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(texts):\n",
    "    data = pd.DataFrame(columns=('Average_Word_Length','Sentence_Count','Word_Count',\n",
    "                                'Character_Count', 'Noun_Count','Adjective_Count',\n",
    "                                'Verb_Count', 'Adverb_Count', 'Lemma_Count' , 'Misspell_Count'\n",
    "                                 ))\n",
    "\n",
    "    data['Average_Word_Length'] = texts.apply(avg_word_len)\n",
    "    data['Sentence_Count'] = texts.apply(sent_count)\n",
    "    data['Word_Count'] = texts.apply(word_count)\n",
    "    data['Character_Count'] = texts.apply(char_count)\n",
    "    temp=texts.apply(count_lemmas)\n",
    "    noun_count,adj_count,verb_count,adverb_count,lemma_count = zip(*temp)\n",
    "    data['Noun_Count'] = noun_count\n",
    "    data['Adjective_Count'] = adj_count\n",
    "    data['Verb_Count'] = verb_count\n",
    "    data['Adverb_Count'] = adverb_count\n",
    "    data['Lemma_Count'] = lemma_count\n",
    "    data['Misspell_Count'] = texts.apply(misspell_count)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(texts,scores):\n",
    "    #create features from the texts and clean non graded essays\n",
    "    data = create_features(texts)\n",
    "    data.describe()\n",
    "    t1=np.where(np.asanyarray(np.isnan(scores)))\n",
    "    scores=scores.drop(scores.index[t1])\n",
    "    data=data.drop(scores.index[t1])\n",
    "    \n",
    "    #scaler = MinMaxScaler()\n",
    "    #data = scaler.fit_transform(data)\n",
    "\n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size = 0.3)\n",
    "\n",
    "    #checking is there any nan cells\n",
    "    print(np.any(np.isnan(scores)))\n",
    "    print(np.all(np.isfinite(scores)))\n",
    "    return X_train, X_test, y_train, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regression(X_train,y_train,X_test,y_test):\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The mean squared error\n",
    "    mse=mean_squared_error(y_test, y_pred)\n",
    "    mse_per= 100*mse/3\n",
    "    print(\"Mean squared error: {}\".format(mse))\n",
    "    print(\"Mean squared error in percentage: {}\".format(mse_per))\n",
    "    #explained variance score\n",
    "    print('Variance score: {}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 1-8\n",
    "# 9:all sets combined\n",
    "texts, scores = select_set(dataframe,1)\n",
    "# get_hist_avg(scores,5)\n",
    "X_train, X_test, y_train, y_test, data = data_prepare(texts,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for Linear Regression \n",
      "\n",
      "Mean squared error: 0.03955152150487646\n",
      "Mean squared error in percentage: 1.3183840501625486\n",
      "Variance score: 0.7069499262501784\n"
     ]
    }
   ],
   "source": [
    "print('Testing for Linear Regression \\n')\n",
    "# lin_regression(X_train,y_train,X_test,y_test)\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The mean squared error\n",
    "mse=mean_squared_error(y_test, y_pred)\n",
    "mse_per= 100*mse/3\n",
    "print(\"Mean squared error: {}\".format(mse))\n",
    "print(\"Mean squared error in percentage: {}\".format(mse_per))\n",
    "#explained variance score\n",
    "print('Variance score: {}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):\n",
    "    text = pd.Series(text)\n",
    "    text = create_features(text)\n",
    "    result = regr.predict(text)[0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0520967125290177\n",
      "2.6428213588460525\n",
      "1.3589641359255733\n",
      "1.3067274047407462\n",
      "3.1603094510224405\n"
     ]
    }
   ],
   "source": [
    "# # New test case\n",
    "# case1 = \"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\"\n",
    "# case2 = \"\"\"Dear Local Newspaper, @CAPS1 I have found that many experts say that computers do not benifit our society. In some cases this is true but in most cases studdies show that computers can help people. While nothing beats talking in person computers can get really close such examples are webcams or e-mail. @PERCENT1 of students who get good grades refer to reliable websites for reasearch or to help find good books. Also online catalouges or advertisments help the economy of stores worldwide. @CAPS2 people were not allowed to use computers most of the modern would not exist. @PERSON1 said that the best form of modern communication is the computer because of the ability to write, talk, or write back for much cheaper! Almost every single event i go to is planed on a computer by communication such as e-mail \"@CAPS2 a student ever needs homework because lam out sick or needs help studying for a test then contact their teacher through the best form of communication for them always e-mail. Even the post office uses computers to get letters and boxes to people. The president of the post office, @PERSON2 said \"@CAPS3 would be imposible to get mail to our coustmers @CAPS2 @CAPS3 were not for computers telling us where a zip code is or how heavy a box is.\" @CAPS4 that tell people what is happening around the world would not exist @CAPS2 @CAPS3 were not for the moder communication abilities that computer provid us. Because information can be commucated so quick. so can reasearch. When the country of @LOCATION2 took a pole @PERCENT2 of people used computer for any type of reasearch, of those @PERCENT3 were students currently in school and @PERCENT4 of them have good grades. When the same survey was taken in the @LOCATION1 @PERCENT5 of people used computers fore reasons and @PERCENT2 were students who had good grade @CAPS2 @CAPS3 were not posible for me to access documents in the @CAPS5 @CAPS6 online I probably would not have gotten an A+ on my @CAPS7 assignment! Could you amagine @CAPS2 suddenly your Newspaper reporters couldn't use the internet to work on their reports? The articles would probably be @NUM1 after the events occur. Most buissness, including the Newspaper, use the internet to advertise, shop, or read. The association of @ORGANIZATION1 reported that in @PERCENT1 of @ORGANIZATION1 used a website and of them @PERCENT5 were in good positions. The president of @CAPS8 @NUM2 imports said that they use an online catalouge because @CAPS3 is cheaper, but they can also promote that @CAPS3 is to save trees, or for the castomer's convinence. Small @ORGANIZATION1 can make websites to promote them selves and explain their star to potential coustomers. @PERSON3, the owner of @ORGANIZATION2's said that the internet saved her resturant. @CAPS2 @CAPS3 wer not for the internet @NUM3 more people would be jobless in @LOCATION3. In conclusion computer help everyday people and without them most convinences would not exist. They help communicate around the world. Computers help people reaserch subjects for school reports, and they make the current economy get better everyday. In moderation computers are the most useful tool out there.\"\"\"\n",
    "# case3 = \"\"\"The affects of the cyclist is if it does not change. it cut hurt a lot of people feeling because they @CAPS1 don’t care about the cyclist. I’m one of them people who does not care about it cause it does not affect me or anyone I know. It is a big deal to write people, some of them @CAPS1 blow stuff up. They talking on tiv and on the radio making all this stuff they say is made up. I don’t believe to I see it. That @CAPS1 not for black people because it has not did anything to us. We really don’t care about the affects of the cyclist.\"\"\"\n",
    "# case4 = \"\"\"Understanding, realitic, mode-all are reason’s how setting affects the setting. If a book has a good setting then the book might be pretty good. When you have a setting it’s more understable of what they are trying today. And if you have a good setting it makes it more realitic So when you read it you wont be thrown off, the mode of the story sets it all off, because usually books have there ups and downs. That’s why they chase the mode.  \"\"\"\n",
    "# case5 = \"\"\" Bell rings.  Shuffle, shuffle. @CAPS1. Snap. EEEE. Crack. Slam. Click, stomp, @CAPS1. Tap tap tap. SLAM. Creak. Shoof, shoof.  Sigh. Seventh class of the day. Here we go. \"@CAPS2! Tu va ou pas? On a +¬tude cette class-l+á. Tu peux aller au bibliotheque si tu veux....\" @CAPS3 all blinked at me, @PERSON1, @NUM1le and @ORGANIZATION1, chocolate-haired and mocha skinned, impatiently awaiting my answer. The truth was, I knew @CAPS3 didn't really care if I came or not. It made no difference to them if I trailed a few feet behind like some pathetic puppy. I was silent but adorable, loved only because I was an @CAPS4. Because I spoke fidgety @CAPS5. Because I was the exchange student, because my translator and colorful clothes were so shocking for ten seconds, and were then forgotten about.  I was a flock of seagulls haircut. So why are you here? I thought. Why did you go on exchange at all? You are the complete opposite of everyone here. No one wants you. Just go home.  But my ego had a ready answer. You begged for this remember? For months and months, it was all you wanted, all you thought about, all you dreamt about. So I went with the girls. As expected, @CAPS3 walked down the three-person wide staircase side-by-side, and I shuffled awkwardly behind them. Finally arriving at @NUM2scalier, we sat at a table, the three girls talking. I glazed my eyes over, attempting to look lost in thought, as if I didn't care I wasn't included. Selfish thoughts buzzed in my head; if @CAPS3 weren't talking to me, why should I make the effort to talk to them?   I really had no idea how @CAPS3 felt about me. How does someone feel about their shadow? @CAPS3 notice it, sure, but it never offers up insight, it never makes you laugh. It's all in the confidence, said my mother's voice, all how you carry yourself. But I knew it wasn't that simple. I was just too alien. These girls would never understand me, as I would never understand them. In frustration, I started to flick peas across the room with my spoon. Pat, flick, sproing.  This caught the interest of @PERSON1, as @NUM1le and @ORGANIZATION1 were discussing something very emotional. Tears began to pour out of @ORGANIZATION1's eyes. Sniffling, she and @NUM1le went to the bathroom, leaving me all alone with @PERSON1. Only @PERSON2 could have felt my felt my same emotion as he stared up at @CAPS6. Silently, I continued shooting peas. @PERSON1 just stared at them as @CAPS3 darted around the room. Suddenly, with a horrible miscalculation, a pea hit a boy in the face. And then, he turned around and swore. And then, @PERSON1 and I looked at each other from across the table.  And then, we laughed.  We laughed so hard I cried. So hard that huge, alien tears flooded from my eyes. People around us were laughing too, even though @CAPS3 had no idea what was so funny. I didn't even know what was so funny. But it didn't matter, because we were dripping tears and snot, reaching for each other, reenacting the pea hitting the boy's face. It was as if we had been friends for years, and laughing happened all the time. It was saturated with all the angst and lonliness and despair I had felt the past four weeks. The connection we felt was instantaneous, like lightening, the kind of connection I felt with my best friends back home. I felt that huge swelling sensation in my chest, like a balloon was stuck inside. My stomach was aching and my cheeks were so sore I felt them seizing up. My heart felt whole even for that second. My soul was open. It was the best laugh of my life.  Sniffle sniffle. GASP. Laughter. GASP. Swipe of tears. Sniffle sniffle. Laughter. GASP. This is why. I thought. This is why you came.  Bell rings.\"\"\"\n",
    "\n",
    "# prediction(case1)\n",
    "# prediction(case2)\n",
    "# prediction(case3)\n",
    "# prediction(case4)\n",
    "# prediction(case5)\n",
    "\n",
    "# # if result < 2:\n",
    "# #     print(\"Bad\")\n",
    "# # elif result < 3:\n",
    "# #     print(\"Average\")\n",
    "# # elif result < 4:\n",
    "# #     print(\"Good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"predictModel.sav\", \"wb\") as file:\n",
    "    pickle.dump(regr, file)\n",
    "\n",
    "with open('predictModel.sav', 'rb') as file:\n",
    "    MODEL = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0520967125290177"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = pd.Series(case1)\n",
    "pred_text = create_features(pred_text)\n",
    "MODEL.predict(pred_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
